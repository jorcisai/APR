{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S1. Red convolucional para MNIST\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en la anterior práctica, primero importamos el conjunto de MNIST y lo normalizamos, pero sin convertir las imágenes en vectores unidimensionales, ya que vamos a trabajar con redes convolucionales que explotan la estructura 2D de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set (60000, 28, 28)\n",
      "test set (10000, 28, 28)\n",
      "training set (48000, 28, 28)\n",
      "val set (12000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "## Importar y normalizar datos\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('test set', x_test.shape)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize [0..255]-->[0..1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes=10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training set', x_train.shape)\n",
    "print('val set', x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo base\n",
    " Partiremos de una topología base que toma la red MLP de la última sesión de la primera práctica (dos capas densas de 1024 neuronas), y le incorpora un par de capas convolucionales cada una seguida por average pooling inspirada en la arquitectura LeNet (1998) propuesta por Yann LeCun para MNIST. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9205\n",
      "Epoch 1: val_accuracy improved from -inf to 0.96925, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 5ms/step - loss: 0.2707 - accuracy: 0.9205 - val_loss: 0.1027 - val_accuracy: 0.9693 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9758\n",
      "Epoch 2: val_accuracy improved from 0.96925 to 0.97875, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0786 - accuracy: 0.9758 - val_loss: 0.0689 - val_accuracy: 0.9787 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "364/375 [============================>.] - ETA: 0s - loss: 0.0536 - accuracy: 0.9842\n",
      "Epoch 3: val_accuracy improved from 0.97875 to 0.98475, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0533 - accuracy: 0.9842 - val_loss: 0.0532 - val_accuracy: 0.9847 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9862\n",
      "Epoch 4: val_accuracy improved from 0.98475 to 0.98500, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0430 - accuracy: 0.9863 - val_loss: 0.0500 - val_accuracy: 0.9850 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9893\n",
      "Epoch 5: val_accuracy improved from 0.98500 to 0.98675, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0330 - accuracy: 0.9893 - val_loss: 0.0432 - val_accuracy: 0.9868 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9914\n",
      "Epoch 6: val_accuracy improved from 0.98675 to 0.98817, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 0.0415 - val_accuracy: 0.9882 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9928\n",
      "Epoch 7: val_accuracy improved from 0.98817 to 0.98942, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0223 - accuracy: 0.9928 - val_loss: 0.0402 - val_accuracy: 0.9894 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9928\n",
      "Epoch 8: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0505 - val_accuracy: 0.9865 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9944\n",
      "Epoch 9: val_accuracy improved from 0.98942 to 0.99058, saving model to best_model.h5\n",
      "375/375 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.0414 - val_accuracy: 0.9906 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 10: val_accuracy improved from 0.99058 to 0.99183, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0345 - val_accuracy: 0.9918 - lr: 2.0000e-04\n",
      "Epoch 11/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 11: val_accuracy improved from 0.99183 to 0.99200, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0356 - val_accuracy: 0.9920 - lr: 2.0000e-04\n",
      "Epoch 12/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 12: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0384 - val_accuracy: 0.9918 - lr: 2.0000e-04\n",
      "Epoch 13/25\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 13: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0387 - val_accuracy: 0.9917 - lr: 4.0000e-05\n",
      "Epoch 14/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 14: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9919 - lr: 4.0000e-05\n",
      "Epoch 15/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 15: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0389 - val_accuracy: 0.9915 - lr: 1.0000e-05\n",
      "Epoch 16/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 16: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0389 - val_accuracy: 0.9918 - lr: 1.0000e-05\n",
      "Epoch 17/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 17: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0392 - val_accuracy: 0.9918 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 18: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0394 - val_accuracy: 0.9917 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 19: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0396 - val_accuracy: 0.9918 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 20: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0400 - val_accuracy: 0.9919 - lr: 1.0000e-05\n",
      "Epoch 21/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 21: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0401 - val_accuracy: 0.9918 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "364/375 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 22: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0404 - val_accuracy: 0.9919 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 23: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0407 - val_accuracy: 0.9920 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "359/375 [===========================>..] - ETA: 0s - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 24: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 9.8506e-04 - accuracy: 0.9998 - val_loss: 0.0411 - val_accuracy: 0.9919 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 9.5023e-04 - accuracy: 0.9998\n",
      "Epoch 25: val_accuracy did not improve from 0.99200\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 9.4697e-04 - accuracy: 0.9998 - val_loss: 0.0412 - val_accuracy: 0.9919 - lr: 1.0000e-05\n",
      "Test loss: 3.08\n",
      "Test accuracy: 99.10\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((28,28,1)))\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=25\n",
    "batch_size=128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr,checkpoint])  \n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio:\n",
    "\n",
    "Probar las técnicas presentadas en la práctica 1 (dropout, batchnorm y aumento de datos) para obtener un acierto en test > 99%, incluso mejor que la obtenida con redes MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización l2 (o l1)\n",
    "\n",
    "La regularización l2 consiste en añadir a la función de coste una penalización proporcional a la norma l2 de los pesos del modelo. De esta forma, se penaliza a los pesos que tengan un valor alto, forzando a que los pesos tengan valores pequeños. Esto se conoce como regularización l2. También podríamos hacer lo mismo con regularización l1 o con ambas (lo que se conoce como *Elastic net*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 1.4852 - accuracy: 0.8688\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93367, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 1.4626 - accuracy: 0.8700 - val_loss: 0.4141 - val_accuracy: 0.9337 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "357/375 [===========================>..] - ETA: 0s - loss: 0.3290 - accuracy: 0.9476\n",
      "Epoch 2: val_accuracy improved from 0.93367 to 0.95400, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3269 - accuracy: 0.9477 - val_loss: 0.2779 - val_accuracy: 0.9540 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.9583\n",
      "Epoch 3: val_accuracy improved from 0.95400 to 0.96458, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.2534 - accuracy: 0.9585 - val_loss: 0.2249 - val_accuracy: 0.9646 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.2112 - accuracy: 0.9636\n",
      "Epoch 4: val_accuracy improved from 0.96458 to 0.97208, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.2109 - accuracy: 0.9636 - val_loss: 0.1893 - val_accuracy: 0.9721 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1860 - accuracy: 0.9697\n",
      "Epoch 5: val_accuracy did not improve from 0.97208\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1860 - accuracy: 0.9696 - val_loss: 0.1748 - val_accuracy: 0.9712 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1692 - accuracy: 0.9714\n",
      "Epoch 6: val_accuracy improved from 0.97208 to 0.97650, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1691 - accuracy: 0.9715 - val_loss: 0.1555 - val_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9726\n",
      "Epoch 7: val_accuracy did not improve from 0.97650\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1606 - accuracy: 0.9726 - val_loss: 0.1684 - val_accuracy: 0.9706 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "360/375 [===========================>..] - ETA: 0s - loss: 0.1467 - accuracy: 0.9754\n",
      "Epoch 8: val_accuracy improved from 0.97650 to 0.97850, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1472 - accuracy: 0.9752 - val_loss: 0.1432 - val_accuracy: 0.9785 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9767\n",
      "Epoch 9: val_accuracy did not improve from 0.97850\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1387 - accuracy: 0.9767 - val_loss: 0.1419 - val_accuracy: 0.9753 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9780\n",
      "Epoch 10: val_accuracy did not improve from 0.97850\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1322 - accuracy: 0.9781 - val_loss: 0.1358 - val_accuracy: 0.9760 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9784\n",
      "Epoch 11: val_accuracy improved from 0.97850 to 0.97933, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1277 - accuracy: 0.9784 - val_loss: 0.1304 - val_accuracy: 0.9793 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9790\n",
      "Epoch 12: val_accuracy did not improve from 0.97933\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1229 - accuracy: 0.9789 - val_loss: 0.1394 - val_accuracy: 0.9752 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9801\n",
      "Epoch 13: val_accuracy improved from 0.97933 to 0.98183, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1210 - accuracy: 0.9799 - val_loss: 0.1198 - val_accuracy: 0.9818 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9800\n",
      "Epoch 14: val_accuracy did not improve from 0.98183\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1147 - accuracy: 0.9800 - val_loss: 0.1314 - val_accuracy: 0.9747 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9820\n",
      "Epoch 15: val_accuracy did not improve from 0.98183\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1083 - accuracy: 0.9818 - val_loss: 0.1153 - val_accuracy: 0.9797 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9814\n",
      "Epoch 16: val_accuracy did not improve from 0.98183\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1057 - accuracy: 0.9814 - val_loss: 0.1140 - val_accuracy: 0.9785 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "361/375 [===========================>..] - ETA: 0s - loss: 0.1038 - accuracy: 0.9812\n",
      "Epoch 17: val_accuracy did not improve from 0.98183\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1036 - accuracy: 0.9812 - val_loss: 0.1166 - val_accuracy: 0.9797 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "149/375 [==========>...................] - ETA: 1s - loss: 0.0993 - accuracy: 0.9824"
     ]
    }
   ],
   "source": [
    "## Teniendo en cuenta el modelo base añade regularización L2 a las capas densas\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((28,28,1)))\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=1024, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=25\n",
    "batch_size=128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr,checkpoint])  \n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "El dropout es una técnica de regularización que consiste en eliminar aleatoriamente un porcentaje de las neuronas de la red durante el entrenamiento. De esta forma, se evita que la red se sobreajuste a los datos de entrenamiento y se mejora la generalización del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.8968\n",
      "Epoch 1: val_accuracy improved from -inf to 0.96983, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3254 - accuracy: 0.8983 - val_loss: 0.1076 - val_accuracy: 0.9698 - lr: 0.0100\n",
      "Epoch 2/25\n",
      " 53/375 [===>..........................] - ETA: 0s - loss: 0.1666 - accuracy: 0.9562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorcisai/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/375 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9571\n",
      "Epoch 2: val_accuracy improved from 0.96983 to 0.97633, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.1620 - accuracy: 0.9571 - val_loss: 0.0990 - val_accuracy: 0.9763 - lr: 0.0100\n",
      "Epoch 3/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.1432 - accuracy: 0.9632\n",
      "Epoch 3: val_accuracy did not improve from 0.97633\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1431 - accuracy: 0.9633 - val_loss: 0.0989 - val_accuracy: 0.9756 - lr: 0.0100\n",
      "Epoch 4/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1420 - accuracy: 0.9648\n",
      "Epoch 4: val_accuracy improved from 0.97633 to 0.97950, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1420 - accuracy: 0.9647 - val_loss: 0.0819 - val_accuracy: 0.9795 - lr: 0.0100\n",
      "Epoch 5/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9659\n",
      "Epoch 5: val_accuracy did not improve from 0.97950\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1421 - accuracy: 0.9657 - val_loss: 0.0933 - val_accuracy: 0.9787 - lr: 0.0100\n",
      "Epoch 6/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9661\n",
      "Epoch 6: val_accuracy did not improve from 0.97950\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.1410 - accuracy: 0.9661 - val_loss: 0.0828 - val_accuracy: 0.9791 - lr: 0.0100\n",
      "Epoch 7/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9784\n",
      "Epoch 7: val_accuracy improved from 0.97950 to 0.98508, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0818 - accuracy: 0.9784 - val_loss: 0.0571 - val_accuracy: 0.9851 - lr: 0.0020\n",
      "Epoch 8/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.0618 - accuracy: 0.9831\n",
      "Epoch 8: val_accuracy improved from 0.98508 to 0.98725, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0621 - accuracy: 0.9830 - val_loss: 0.0531 - val_accuracy: 0.9872 - lr: 0.0020\n",
      "Epoch 9/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9853\n",
      "Epoch 9: val_accuracy improved from 0.98725 to 0.98792, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0550 - accuracy: 0.9853 - val_loss: 0.0521 - val_accuracy: 0.9879 - lr: 0.0020\n",
      "Epoch 10/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9857\n",
      "Epoch 10: val_accuracy did not improve from 0.98792\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0500 - accuracy: 0.9857 - val_loss: 0.0528 - val_accuracy: 0.9875 - lr: 0.0020\n",
      "Epoch 11/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9874\n",
      "Epoch 11: val_accuracy did not improve from 0.98792\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0443 - accuracy: 0.9874 - val_loss: 0.0522 - val_accuracy: 0.9879 - lr: 0.0020\n",
      "Epoch 12/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0343 - accuracy: 0.9898\n",
      "Epoch 12: val_accuracy improved from 0.98792 to 0.98892, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0344 - accuracy: 0.9898 - val_loss: 0.0504 - val_accuracy: 0.9889 - lr: 4.0000e-04\n",
      "Epoch 13/25\n",
      "361/375 [===========================>..] - ETA: 0s - loss: 0.0365 - accuracy: 0.9893\n",
      "Epoch 13: val_accuracy did not improve from 0.98892\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0359 - accuracy: 0.9895 - val_loss: 0.0502 - val_accuracy: 0.9887 - lr: 4.0000e-04\n",
      "Epoch 14/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9907\n",
      "Epoch 14: val_accuracy improved from 0.98892 to 0.98925, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0320 - accuracy: 0.9907 - val_loss: 0.0506 - val_accuracy: 0.9893 - lr: 4.0000e-04\n",
      "Epoch 15/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0316 - accuracy: 0.9906\n",
      "Epoch 15: val_accuracy did not improve from 0.98925\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.0499 - val_accuracy: 0.9887 - lr: 4.0000e-04\n",
      "Epoch 16/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9911\n",
      "Epoch 16: val_accuracy did not improve from 0.98925\n",
      "375/375 [==============================] - 1s 4ms/step - loss: 0.0302 - accuracy: 0.9911 - val_loss: 0.0501 - val_accuracy: 0.9884 - lr: 4.0000e-04\n",
      "Epoch 17/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0287 - accuracy: 0.9914\n",
      "Epoch 17: val_accuracy did not improve from 0.98925\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0288 - accuracy: 0.9914 - val_loss: 0.0509 - val_accuracy: 0.9888 - lr: 4.0000e-04\n",
      "Epoch 18/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.0276 - accuracy: 0.9921\n",
      "Epoch 18: val_accuracy improved from 0.98925 to 0.98942, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 0.0502 - val_accuracy: 0.9894 - lr: 8.0000e-05\n",
      "Epoch 19/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9918\n",
      "Epoch 19: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.0499 - val_accuracy: 0.9893 - lr: 8.0000e-05\n",
      "Epoch 20/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9921\n",
      "Epoch 20: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0265 - accuracy: 0.9921 - val_loss: 0.0500 - val_accuracy: 0.9893 - lr: 1.6000e-05\n",
      "Epoch 21/25\n",
      "365/375 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9912\n",
      "Epoch 21: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0283 - accuracy: 0.9912 - val_loss: 0.0499 - val_accuracy: 0.9893 - lr: 1.6000e-05\n",
      "Epoch 22/25\n",
      "357/375 [===========================>..] - ETA: 0s - loss: 0.0279 - accuracy: 0.9917\n",
      "Epoch 22: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0280 - accuracy: 0.9917 - val_loss: 0.0499 - val_accuracy: 0.9893 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "366/375 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9919\n",
      "Epoch 23: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0269 - accuracy: 0.9919 - val_loss: 0.0499 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9921\n",
      "Epoch 24: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0264 - accuracy: 0.9921 - val_loss: 0.0499 - val_accuracy: 0.9894 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9920\n",
      "Epoch 25: val_accuracy did not improve from 0.98942\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.0499 - val_accuracy: 0.9893 - lr: 1.0000e-05\n",
      "Test loss: 4.06\n",
      "Test accuracy: 98.93\n"
     ]
    }
   ],
   "source": [
    "## Teniendo en cuenta el modelo base añade regularización de tipo dropout a las capas densas\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((28,28,1)))\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=25\n",
    "batch_size=128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr,checkpoint])  \n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización BatchNorm\n",
    "\n",
    "La normalización BatchNorm consiste en normalizar la salida de una capa de la red neuronal para que tenga media 0 y varianza 1. De esta forma, se consigue que la red neuronal pueda entrenarse más rápido y que sea más robusta a cambios en los pesos de las capas anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9422\n",
      "Epoch 1: val_accuracy improved from -inf to 0.97050, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 6ms/step - loss: 0.2325 - accuracy: 0.9424 - val_loss: 0.1222 - val_accuracy: 0.9705 - lr: 0.0100\n",
      "Epoch 2/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9765\n",
      "Epoch 2: val_accuracy improved from 0.97050 to 0.97633, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0764 - accuracy: 0.9765 - val_loss: 0.0874 - val_accuracy: 0.9763 - lr: 0.0100\n",
      "Epoch 3/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9831\n",
      "Epoch 3: val_accuracy improved from 0.97633 to 0.98158, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0558 - accuracy: 0.9831 - val_loss: 0.0691 - val_accuracy: 0.9816 - lr: 0.0100\n",
      "Epoch 4/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.0491 - accuracy: 0.9849\n",
      "Epoch 4: val_accuracy improved from 0.98158 to 0.98425, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0493 - accuracy: 0.9849 - val_loss: 0.0576 - val_accuracy: 0.9843 - lr: 0.0100\n",
      "Epoch 5/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9859\n",
      "Epoch 5: val_accuracy did not improve from 0.98425\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0449 - accuracy: 0.9859 - val_loss: 0.1100 - val_accuracy: 0.9745 - lr: 0.0100\n",
      "Epoch 6/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9887\n",
      "Epoch 6: val_accuracy did not improve from 0.98425\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0367 - accuracy: 0.9886 - val_loss: 0.0947 - val_accuracy: 0.9744 - lr: 0.0100\n",
      "Epoch 7/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9946\n",
      "Epoch 7: val_accuracy improved from 0.98425 to 0.99150, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0173 - accuracy: 0.9946 - val_loss: 0.0323 - val_accuracy: 0.9915 - lr: 0.0020\n",
      "Epoch 8/25\n",
      "369/375 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9968\n",
      "Epoch 8: val_accuracy improved from 0.99150 to 0.99167, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0323 - val_accuracy: 0.9917 - lr: 0.0020\n",
      "Epoch 9/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 9: val_accuracy improved from 0.99167 to 0.99208, saving model to best_model.h5\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0335 - val_accuracy: 0.9921 - lr: 0.0020\n",
      "Epoch 10/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9984\n",
      "Epoch 10: val_accuracy improved from 0.99208 to 0.99292, saving model to best_model.h5\n",
      "375/375 [==============================] - 3s 8ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.0311 - val_accuracy: 0.9929 - lr: 4.0000e-04\n",
      "Epoch 11/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9987\n",
      "Epoch 11: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0317 - val_accuracy: 0.9922 - lr: 4.0000e-04\n",
      "Epoch 12/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9988\n",
      "Epoch 12: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0322 - val_accuracy: 0.9926 - lr: 4.0000e-04\n",
      "Epoch 13/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 13: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0314 - val_accuracy: 0.9926 - lr: 8.0000e-05\n",
      "Epoch 14/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 14: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0039 - accuracy: 0.9990 - val_loss: 0.0313 - val_accuracy: 0.9925 - lr: 8.0000e-05\n",
      "Epoch 15/25\n",
      "362/375 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 15: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0317 - val_accuracy: 0.9925 - lr: 1.6000e-05\n",
      "Epoch 16/25\n",
      "371/375 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 16: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0314 - val_accuracy: 0.9927 - lr: 1.6000e-05\n",
      "Epoch 17/25\n",
      "370/375 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 17: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0317 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 18/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 18: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0310 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 19/25\n",
      "367/375 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 19: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 3s 7ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0312 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 20/25\n",
      "368/375 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 20: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0316 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 21/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 21: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0312 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 22/25\n",
      "361/375 [===========================>..] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 22: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0316 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Epoch 23/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 23: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 7ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0314 - val_accuracy: 0.9926 - lr: 1.0000e-05\n",
      "Epoch 24/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 24: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0314 - val_accuracy: 0.9925 - lr: 1.0000e-05\n",
      "Epoch 25/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 25: val_accuracy did not improve from 0.99292\n",
      "375/375 [==============================] - 2s 6ms/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0315 - val_accuracy: 0.9927 - lr: 1.0000e-05\n",
      "Test loss: 2.25\n",
      "Test accuracy: 99.33\n"
     ]
    }
   ],
   "source": [
    "## Teniendo en cuenta el modelo base añade normalización BatchNorm\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((28,28,1)))\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=25\n",
    "batch_size=128\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr,checkpoint])  \n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentado de datos\n",
    "\n",
    "El aumentado de datos consiste en generar nuevos datos de entrenamiento a partir de los datos de entrenamiento originales. De esta forma, se consigue que el modelo sea más robusto y que se generalice mejor a datos que no ha visto durante el entrenamiento.\n",
    "\n",
    "En nuestro caso para los dígitos de la MNIST vamos a realizar un aumento de datos de la siguiente forma:\n",
    "\n",
    "- Rotación aleatoria de la imagen entre -30 y 30 grados.\n",
    "- Traslación aleatoria de la imagen entre -3 y 3 píxeles en horizontal y vertical.\n",
    "- Escalado aleatorio de la imagen entre 0.8 y 1.2.\n",
    "- Inversión aleatoria de la imagen en horizontal y vertical. **NO!!!**\n",
    "\n",
    "El aumentado de datos se ejecuta en CPU y ralentiza el entrenamiento.\n",
    "\n",
    "Normalmente además, se necesitarán más epochs para entrenar el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.8426\n",
      "Epoch 1: val_accuracy improved from -inf to 0.95550, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 16ms/step - loss: 0.5723 - accuracy: 0.8433 - val_loss: 0.1538 - val_accuracy: 0.9555 - lr: 0.0100\n",
      "Epoch 2/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9358\n",
      "Epoch 2: val_accuracy improved from 0.95550 to 0.97275, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.2089 - accuracy: 0.9358 - val_loss: 0.0865 - val_accuracy: 0.9728 - lr: 0.0100\n",
      "Epoch 3/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1593 - accuracy: 0.9518\n",
      "Epoch 3: val_accuracy improved from 0.97275 to 0.97750, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1593 - accuracy: 0.9518 - val_loss: 0.0724 - val_accuracy: 0.9775 - lr: 0.0100\n",
      "Epoch 4/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9575\n",
      "Epoch 4: val_accuracy did not improve from 0.97750\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1387 - accuracy: 0.9575 - val_loss: 0.1458 - val_accuracy: 0.9556 - lr: 0.0100\n",
      "Epoch 5/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9629\n",
      "Epoch 5: val_accuracy improved from 0.97750 to 0.97867, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.1199 - accuracy: 0.9629 - val_loss: 0.0700 - val_accuracy: 0.9787 - lr: 0.0100\n",
      "Epoch 6/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9633\n",
      "Epoch 6: val_accuracy improved from 0.97867 to 0.98150, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.1195 - accuracy: 0.9634 - val_loss: 0.0627 - val_accuracy: 0.9815 - lr: 0.0100\n",
      "Epoch 7/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1093 - accuracy: 0.9661\n",
      "Epoch 7: val_accuracy improved from 0.98150 to 0.98392, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1093 - accuracy: 0.9661 - val_loss: 0.0536 - val_accuracy: 0.9839 - lr: 0.0100\n",
      "Epoch 8/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9670\n",
      "Epoch 8: val_accuracy did not improve from 0.98392\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1062 - accuracy: 0.9670 - val_loss: 0.0991 - val_accuracy: 0.9712 - lr: 0.0100\n",
      "Epoch 9/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.1046 - accuracy: 0.9689\n",
      "Epoch 9: val_accuracy did not improve from 0.98392\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.1045 - accuracy: 0.9689 - val_loss: 0.0680 - val_accuracy: 0.9792 - lr: 0.0100\n",
      "Epoch 10/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9799\n",
      "Epoch 10: val_accuracy improved from 0.98392 to 0.98883, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0661 - accuracy: 0.9798 - val_loss: 0.0383 - val_accuracy: 0.9888 - lr: 0.0020\n",
      "Epoch 11/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9824\n",
      "Epoch 11: val_accuracy did not improve from 0.98883\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0564 - accuracy: 0.9824 - val_loss: 0.0394 - val_accuracy: 0.9887 - lr: 0.0020\n",
      "Epoch 12/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9834\n",
      "Epoch 12: val_accuracy improved from 0.98883 to 0.98900, saving model to best_model.h5\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0536 - accuracy: 0.9834 - val_loss: 0.0398 - val_accuracy: 0.9890 - lr: 0.0020\n",
      "Epoch 13/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9845\n",
      "Epoch 13: val_accuracy improved from 0.98900 to 0.99100, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0495 - accuracy: 0.9845 - val_loss: 0.0303 - val_accuracy: 0.9910 - lr: 4.0000e-04\n",
      "Epoch 14/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0428 - accuracy: 0.9865\n",
      "Epoch 14: val_accuracy improved from 0.99100 to 0.99183, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0428 - accuracy: 0.9865 - val_loss: 0.0291 - val_accuracy: 0.9918 - lr: 4.0000e-04\n",
      "Epoch 15/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9861\n",
      "Epoch 15: val_accuracy did not improve from 0.99183\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0459 - accuracy: 0.9862 - val_loss: 0.0296 - val_accuracy: 0.9916 - lr: 4.0000e-04\n",
      "Epoch 16/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9863\n",
      "Epoch 16: val_accuracy did not improve from 0.99183\n",
      "375/375 [==============================] - 6s 16ms/step - loss: 0.0448 - accuracy: 0.9864 - val_loss: 0.0301 - val_accuracy: 0.9916 - lr: 4.0000e-04\n",
      "Epoch 17/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9860\n",
      "Epoch 17: val_accuracy improved from 0.99183 to 0.99208, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0435 - accuracy: 0.9861 - val_loss: 0.0279 - val_accuracy: 0.9921 - lr: 8.0000e-05\n",
      "Epoch 18/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9868\n",
      "Epoch 18: val_accuracy did not improve from 0.99208\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0418 - accuracy: 0.9869 - val_loss: 0.0279 - val_accuracy: 0.9921 - lr: 8.0000e-05\n",
      "Epoch 19/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9873\n",
      "Epoch 19: val_accuracy improved from 0.99208 to 0.99217, saving model to best_model.h5\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0426 - accuracy: 0.9873 - val_loss: 0.0278 - val_accuracy: 0.9922 - lr: 8.0000e-05\n",
      "Epoch 20/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9869\n",
      "Epoch 20: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0428 - accuracy: 0.9869 - val_loss: 0.0277 - val_accuracy: 0.9920 - lr: 8.0000e-05\n",
      "Epoch 21/25\n",
      "372/375 [============================>.] - ETA: 0s - loss: 0.0426 - accuracy: 0.9866\n",
      "Epoch 21: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0425 - accuracy: 0.9867 - val_loss: 0.0285 - val_accuracy: 0.9919 - lr: 8.0000e-05\n",
      "Epoch 22/25\n",
      "375/375 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9869\n",
      "Epoch 22: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 7s 17ms/step - loss: 0.0400 - accuracy: 0.9869 - val_loss: 0.0281 - val_accuracy: 0.9921 - lr: 8.0000e-05\n",
      "Epoch 23/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9876\n",
      "Epoch 23: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 7s 18ms/step - loss: 0.0411 - accuracy: 0.9876 - val_loss: 0.0278 - val_accuracy: 0.9920 - lr: 1.6000e-05\n",
      "Epoch 24/25\n",
      "373/375 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9876\n",
      "Epoch 24: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0279 - val_accuracy: 0.9921 - lr: 1.6000e-05\n",
      "Epoch 25/25\n",
      "374/375 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9877\n",
      "Epoch 25: val_accuracy did not improve from 0.99217\n",
      "375/375 [==============================] - 6s 17ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0279 - val_accuracy: 0.9922 - lr: 1.0000e-05\n",
      "Test loss: 2.39\n",
      "Test accuracy: 99.32\n"
     ]
    }
   ],
   "source": [
    "## Implementamos en el ejemplo base el aumentado de datos\n",
    "from keras import Sequential\n",
    "from keras.layers import Input, Conv2D, AveragePooling2D, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "## Importante: ImageDataGenerator espera una imagen con 3 canales, necesitamos hacer reshape\n",
    "x_train = x_train.reshape(48000, 28, 28, 1)\n",
    "x_val = x_val.reshape(12000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "## Ajustamos el generador de datos\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input((28,28,1)))\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "epochs=25\n",
    "batch_size=128\n",
    "## Entrenamos con el generador de datos en lugar de con el dataset\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[reduce_lr,checkpoint])\n",
    "\n",
    "## Cargar el mejor modelo y evaluarlo con el test set\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
