{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4. Fine-tuning de modelo pre-entrenado para la clasificación de CIFAR-10\n",
    "En esta sesión veremos como realizar el fine-tuning (ajuste fino) de una arquitectura de red [EfficientNet](https://arxiv.org/abs/1905.11946), más concretamente veremos su [B0 disponible en Keras](https://keras.io/api/applications/efficientnet/#efficientnetb0-function) pre-entrenada con la base de datos de imágenes [ImageNet](https://www.image-net.org) para la clasificación de CIFAR-10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "La utilización de una red pre-entrenada conlleva preprocesar nuestros datos con el mismo preproceso que se empleó para los datos de entrenamiento de la red pre-entrenada. Si realizamos este preproceso para nuestro conjunto de entrenamiento, esto requeriría aproximadamente 40GB, por lo que es conveniente realizar este preproceso bajo demanda. Es decir, aplicaremos el preproceso para cada batch en el momento que vaya a ser utilizado.      \n",
    "\n",
    "La aplicación de este preproceso bajo demanda está ya implementado en el módulo [tensorflow_datasets](https://www.tensorflow.org/datasets), así que en esta sesión cargaremos y manipularemos CIFAR-10 como un objeto [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) utilizando este módulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_data, test_data = tfds.load('cifar10', split=['train', 'test'], as_supervised=True)\n",
    "train_size = len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproceso\n",
    "\n",
    "Primero definimos una función que aplica el preproceso necesario a una muestra de entrenamiento (imagen, etiqueta de clase). Esto incluye redimensionar la imagen a 299 x 299, el preproceso específico de la red InceptionV3 y convertir la etiqueta de clase a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "img_size = (299, 299)\n",
    "num_classes = 10\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_input(image)\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación indicamos que la función anteriormente definida se aplicará a cada imagen cuando sea necesario. Para ello utilizamos la función [map()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) del objeto [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) y le pasamos como parámetro la función de preproceso que queremos que sea aplique a cada muestra del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess)\n",
    "test_data = test_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones [take()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) y [skip()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip) combinadas permiten definir los conjuntos de entrenamiento y validación como nuevos Datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * train_size)\n",
    "train_dataset = train_data.take(train_size)\n",
    "val_dataset = train_data.skip(train_size)\n",
    "test_dataset = test_data\n",
    "\n",
    "print(len(train_dataset),len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo pre-entrenado\n",
    "\n",
    "Seguidamente procedemos con la carga del modelo EfficientNet B0 con los pesos resultantes de entrenarlo con la base de datos Imagenet, pero no queremos que el modelo incluya la capa de salida (include_top=False) que por defecto es una softmax de 1000 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.efficientnet import EfficientNetB0\n",
    "\n",
    "model = EfficientNetB0(input_shape=img_size + (3,),include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del modelo pre-entrenado\n",
    "\n",
    "Vamos a preparar la red EfficientNet B0 para ser entrenada (fine-tuning) con CIFAR-10. Dado el número de parámetros de este modelo (21M), nos limitaremos a utilizarlo con los valores por defecto y añadiremos una capa GlobalAveragePooling + MLP seguida de una softmax de 10 neuronas (10 clases) acorde a CIFAR-10 que sí que entrenaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo con los mismos parámetros que en sesiones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo utilizando los conjuntos de datos organizado en batches y que son cargados en memoria dinámicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 18:00:20.405353: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-12-20 18:00:20.405370: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2023-12-20 18:00:20.405404: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.496199: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.496271: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.498464: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.499140: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.499169: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.499623: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.499674: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.500229: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:20.694124: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "2023-12-20 18:00:20.743621: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-12-20 18:00:21.068758: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.272860: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.272881: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.273232: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.273273: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.274299: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.274491: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.276078: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.277317: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.277927: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.569685: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-20 18:00:21.595806: I external/local_xla/xla/service/service.cc:168] XLA service 0x7efde0fd15b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-20 18:00:21.595825: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2023-12-20 18:00:21.599732: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703091621.636818   23851 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - ETA: 0s - loss: 0.5379 - accuracy: 0.8129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 18:02:11.731385: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88640, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorcisai/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 144s 112ms/step - loss: 0.5379 - accuracy: 0.8129 - val_loss: 0.3308 - val_accuracy: 0.8864 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8523\n",
      "Epoch 2: val_accuracy improved from 0.88640 to 0.89280, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 0.4242 - accuracy: 0.8523 - val_loss: 0.3066 - val_accuracy: 0.8928 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.8661\n",
      "Epoch 3: val_accuracy improved from 0.89280 to 0.89940, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 0.3846 - accuracy: 0.8661 - val_loss: 0.2962 - val_accuracy: 0.8994 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.8742\n",
      "Epoch 4: val_accuracy did not improve from 0.89940\n",
      "1250/1250 [==============================] - 139s 111ms/step - loss: 0.3582 - accuracy: 0.8742 - val_loss: 0.2906 - val_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3412 - accuracy: 0.8798\n",
      "Epoch 5: val_accuracy improved from 0.89940 to 0.90900, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 139s 111ms/step - loss: 0.3412 - accuracy: 0.8798 - val_loss: 0.2707 - val_accuracy: 0.9090 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8846\n",
      "Epoch 6: val_accuracy did not improve from 0.90900\n",
      "1250/1250 [==============================] - 137s 110ms/step - loss: 0.3283 - accuracy: 0.8846 - val_loss: 0.2786 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8922\n",
      "Epoch 7: val_accuracy did not improve from 0.90900\n",
      "1250/1250 [==============================] - 137s 109ms/step - loss: 0.3101 - accuracy: 0.8922 - val_loss: 0.2762 - val_accuracy: 0.9088 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2438 - accuracy: 0.9137\n",
      "Epoch 8: val_accuracy improved from 0.90900 to 0.91900, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 137s 110ms/step - loss: 0.2438 - accuracy: 0.9137 - val_loss: 0.2501 - val_accuracy: 0.9190 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2213 - accuracy: 0.9230\n",
      "Epoch 9: val_accuracy did not improve from 0.91900\n",
      "1250/1250 [==============================] - 137s 110ms/step - loss: 0.2213 - accuracy: 0.9230 - val_loss: 0.2499 - val_accuracy: 0.9178 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9252\n",
      "Epoch 10: val_accuracy improved from 0.91900 to 0.92050, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 137s 110ms/step - loss: 0.2122 - accuracy: 0.9252 - val_loss: 0.2452 - val_accuracy: 0.9205 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=10\n",
    "batch_size=32\n",
    "train_dataset_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset_batched = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "history = model.fit(train_dataset_batched,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_dataset_batched,\n",
    "                    callbacks=[reduce_lr,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el mejor modelo y evaluarlo con el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 26.79\n",
      "Test accuracy: 91.35\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.h5')\n",
    "test_dataset_batched = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "score = model.evaluate(test_dataset_batched, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
