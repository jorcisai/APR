{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S4. Fine-tuning de modelo pre-entrenado para la clasificación de CIFAR-100\n",
    "En esta sesión veremos como realizar el fine-tuning (ajuste fino) de una arquitectura de red [Inception](https://paperswithcode.com/paper/rethinking-the-inception-architecture-for), más concretamente veremos su [versión 3 (V3) disponible en Keras](https://keras.io/api/applications/inceptionv3/) pre-entrenada con la base de datos de imágenes [ImageNet](https://www.image-net.org) para la clasificación de CIFAR-100. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo preentrenado: Inception\n",
    "\n",
    "La red Inception de Google (2015) basado en CNNs fue uno de los primeros modelos pre-entrenados pensados para que fuera aplicado a otras tareas (*transfer learning*) tras un ajuste fino del modelo. Estos modelos pre-entrenados presentan la arquitectura típica **stem-body-head:**\n",
    "* **Stem (raíz):** $\\;$ dos o tres capas convolucionales que extraen características de bajo nivel\n",
    "* **Body (cuerpo):** $\\;$ subred de **bloques convolucionales** repetidos\n",
    "* **Head (cabeza):** $\\;$ transforma la salida del cuerpo mediante un red densa según la tarea a abordar (clasificación, segmentación, etc.)\n",
    "\n",
    "<div align=\"center\">\n",
    "<table><tr>\n",
    "<td style=\"border: none;\"><img src=\"Figure_14.19.png\"/></td>\n",
    "</tr></table>\n",
    "</div>\n",
    "\n",
    "Esta red toma su nombre de los bloque Inception que se definen en su cuerpo. En cada bloque Inception se concatenan ramas paralelas con kernels diferentes, para que cada capa del cuerpo escoja la que más le convenga\n",
    "<div align=\"center\">\n",
    "<table><tr>\n",
    "<td style=\"border: none;\"><img src=\"Figure_14.18.png\" width=600/></td>\n",
    "</tr></table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "La utilización de una red pre-entrenada conlleva preprocesar nuestros datos con el mismo preproceso que se empleó para los datos de entrenamiento de la red pre-entrenada. En el caso de la red Inception V3, las imágenes necesitan, entre otras cosas, ser redimensionadas a 299 x 299. Si realizamos este preproceso para nuestro conjunto de entrenamiento, esto requeriría aproximadamente 40GB, por lo que es conveniente realizar este preproceso bajo demanda. Es decir, aplicaremos el preproceso para cada batch en el momento que vaya a ser utilizado.      \n",
    "\n",
    "La aplicación de este preproceso bajo demanda está ya implementado en el módulo [tensorflow_datasets](https://www.tensorflow.org/datasets), así que en esta sesión cargaremos y manipularemos CIFAR-100 como un objeto [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) utilizando este módulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "train_data, test_data = tfds.load('cifar100', split=['train', 'test'], as_supervised=True)\n",
    "train_size = len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproceso\n",
    "\n",
    "Primero definimos una función que aplica el preproceso necesario a una muestra de entrenamiento (imagen, etiqueta de clase). Esto incluye redimensionar la imagen a 299 x 299, el preproceso específico de la red InceptionV3 y convertir la etiqueta de clase a one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "img_size = (299, 299)\n",
    "num_classes = 100\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = preprocess_input(image)\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación indicamos que la función anteriormente definida se aplicará a cada imagen cuando sea necesario. Para ello utilizamos la función [map()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) del objeto [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) y le pasamos como parámetro la función de preproceso que queremos que sea aplique a cada muestra del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess)\n",
    "test_data = test_data.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones [take()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) y [skip()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip) combinadas permiten definir los conjuntos de entrenamiento y validación como nuevos Datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * train_size)\n",
    "train_dataset = train_data.take(train_size)\n",
    "val_dataset = train_data.skip(train_size)\n",
    "test_dataset = test_data\n",
    "\n",
    "print(len(train_dataset),len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga del modelo pre-entrenado\n",
    "\n",
    "Seguidamente procedemos con la carga del modelo Inception V3 con los pesos resultantes de entrenarlo con la base de datos Imagenet, pero no queremos que el modelo incluya la capa de salida (include_top=False) que por defecto es una softmax de 1000 clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "model = InceptionV3(input_shape=img_size + (3,),include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del modelo pre-entrenado\n",
    "\n",
    "Vamos a preparar la red Inception V3 para ser entrenada (fine-tuning) con CIFAR-100. Dado el número de parámetros de este modelo (21M), nos limitaremos a utilizarlo con los valores por defecto y añadiremos una capa GlobalAveragePooling + MLP seguida de una softmax de 100 neuronas (100 clases) acorde a CIFAR-100 que sí que entrenaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo con los mismos parámetros que en sesiones anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo utilizando los conjuntos de datos organizado en batches y que son cargados en memoria dinámicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 15:25:10.400179: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
      "2023-12-19 15:25:10.510074: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:225] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
      "2023-12-19 15:25:10.510087: W external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:228] Used ptxas at ptxas\n",
      "2023-12-19 15:25:10.510126: W external/local_xla/xla/stream_executor/gpu/redzone_allocator.cc:322] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-12-19 15:25:16.234261: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.379626: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.381103: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.381201: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.381816: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.381847: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.381872: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.382864: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.384802: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
      "2023-12-19 15:25:16.385483: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:191] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1250 [..............................] - ETA: 2:56:50 - loss: 4.9140 - accuracy: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 15:25:16.656396: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fbf4cc0d430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-19 15:25:16.656413: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2023-12-19 15:25:16.662957: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1702995916.710413    6953 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - ETA: 0s - loss: 2.2722 - accuracy: 0.4182"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 15:26:53.865320: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56360, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorcisai/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 136s 102ms/step - loss: 2.2722 - accuracy: 0.4182 - val_loss: 1.5402 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.6989 - accuracy: 0.5308\n",
      "Epoch 2: val_accuracy improved from 0.56360 to 0.59640, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 1.6989 - accuracy: 0.5308 - val_loss: 1.4305 - val_accuracy: 0.5964 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.5728 - accuracy: 0.5612\n",
      "Epoch 3: val_accuracy improved from 0.59640 to 0.61240, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 1.5728 - accuracy: 0.5612 - val_loss: 1.3723 - val_accuracy: 0.6124 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4953 - accuracy: 0.5813\n",
      "Epoch 4: val_accuracy improved from 0.61240 to 0.61890, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 1.4953 - accuracy: 0.5813 - val_loss: 1.3553 - val_accuracy: 0.6189 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.4364 - accuracy: 0.5933\n",
      "Epoch 5: val_accuracy did not improve from 0.61890\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 1.4364 - accuracy: 0.5933 - val_loss: 1.4086 - val_accuracy: 0.6101 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.3819 - accuracy: 0.6063\n",
      "Epoch 6: val_accuracy improved from 0.61890 to 0.62060, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 1.3819 - accuracy: 0.6063 - val_loss: 1.3635 - val_accuracy: 0.6206 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.6692\n",
      "Epoch 7: val_accuracy improved from 0.62060 to 0.65970, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 1.1247 - accuracy: 0.6692 - val_loss: 1.2504 - val_accuracy: 0.6597 - lr: 2.0000e-04\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0452 - accuracy: 0.6908\n",
      "Epoch 8: val_accuracy improved from 0.65970 to 0.66380, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 1.0452 - accuracy: 0.6908 - val_loss: 1.2391 - val_accuracy: 0.6638 - lr: 2.0000e-04\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 1.0021 - accuracy: 0.7022\n",
      "Epoch 9: val_accuracy did not improve from 0.66380\n",
      "1250/1250 [==============================] - 122s 98ms/step - loss: 1.0021 - accuracy: 0.7022 - val_loss: 1.2372 - val_accuracy: 0.6606 - lr: 2.0000e-04\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.7102\n",
      "Epoch 10: val_accuracy improved from 0.66380 to 0.66610, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.9702 - accuracy: 0.7102 - val_loss: 1.2283 - val_accuracy: 0.6661 - lr: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=10\n",
    "batch_size=32\n",
    "train_dataset_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset_batched = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "history = model.fit(train_dataset_batched,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_dataset_batched,\n",
    "                    callbacks=[reduce_lr,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el mejor modelo y evaluarlo con el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 122.80\n",
      "Test accuracy: 66.07\n"
     ]
    }
   ],
   "source": [
    "model = load_model('best_model.h5')\n",
    "test_dataset_batched = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "score = model.evaluate(test_dataset_batched, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumento de datos\n",
    "\n",
    "En las sesiones anteriores, hemos utilizado la función [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) para realizar el aumento de datos. Sin embargo, esta función no se recomienda para los nuevos desarrollos de código por estar obsoleta, y en su lugar se deben utilizar las [capas de preproceso](https://www.tensorflow.org/guide/keras/preprocessing_layers). Más concretamente, utilizaremos algunas de las [capas de preproceso de aumento de datos para imágenes](https://www.tensorflow.org/guide/keras/preprocessing_layers#image_data_augmentation).\n",
    "\n",
    "Añadiremos estas capas de aumento de datos antes del modelo Inception V3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, RandomRotation, RandomTranslation, RandomZoom\n",
    "from keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=img_size + (3,))\n",
    "\n",
    "x = RandomRotation(factor=0.1, fill_mode='nearest')(input_layer)\n",
    "x = RandomTranslation(height_factor=0.1, width_factor=0.1, fill_mode='nearest')(x)\n",
    "x = RandomZoom(height_factor=0.2, fill_mode='nearest')(x)\n",
    "\n",
    "inception_model = InceptionV3(input_shape=img_size + (3,),include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in inception_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = inception_model(x)\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "aug_model = Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt=Adam(learning_rate=0.001)\n",
    "aug_model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.9098 - accuracy: 0.2760\n",
      "Epoch 1: val_accuracy improved from -inf to 0.49830, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 132s 104ms/step - loss: 2.9098 - accuracy: 0.2760 - val_loss: 1.8095 - val_accuracy: 0.4983 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.3303 - accuracy: 0.3827\n",
      "Epoch 2: val_accuracy improved from 0.49830 to 0.54270, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 128s 102ms/step - loss: 2.3303 - accuracy: 0.3827 - val_loss: 1.6522 - val_accuracy: 0.5427 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.2392 - accuracy: 0.4059\n",
      "Epoch 3: val_accuracy improved from 0.54270 to 0.55660, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 128s 103ms/step - loss: 2.2392 - accuracy: 0.4059 - val_loss: 1.5704 - val_accuracy: 0.5566 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.2053 - accuracy: 0.4154\n",
      "Epoch 4: val_accuracy improved from 0.55660 to 0.56520, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.2053 - accuracy: 0.4154 - val_loss: 1.5404 - val_accuracy: 0.5652 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.1401 - accuracy: 0.4278\n",
      "Epoch 5: val_accuracy improved from 0.56520 to 0.57540, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.1401 - accuracy: 0.4278 - val_loss: 1.4899 - val_accuracy: 0.5754 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.1276 - accuracy: 0.4294\n",
      "Epoch 6: val_accuracy improved from 0.57540 to 0.58250, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.1276 - accuracy: 0.4294 - val_loss: 1.4898 - val_accuracy: 0.5825 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.1064 - accuracy: 0.4360\n",
      "Epoch 7: val_accuracy improved from 0.58250 to 0.58600, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.1064 - accuracy: 0.4360 - val_loss: 1.4554 - val_accuracy: 0.5860 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.0953 - accuracy: 0.4434\n",
      "Epoch 8: val_accuracy did not improve from 0.58600\n",
      "1250/1250 [==============================] - 128s 103ms/step - loss: 2.0953 - accuracy: 0.4434 - val_loss: 1.4732 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.0696 - accuracy: 0.4459\n",
      "Epoch 9: val_accuracy improved from 0.58600 to 0.58950, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.0696 - accuracy: 0.4459 - val_loss: 1.4505 - val_accuracy: 0.5895 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - ETA: 0s - loss: 2.0689 - accuracy: 0.4479\n",
      "Epoch 10: val_accuracy improved from 0.58950 to 0.59180, saving model to best_model.h5\n",
      "1250/1250 [==============================] - 129s 103ms/step - loss: 2.0689 - accuracy: 0.4479 - val_loss: 1.4436 - val_accuracy: 0.5918 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "epochs=10\n",
    "batch_size=32\n",
    "train_dataset_batched = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset_batched = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "history = aug_model.fit(train_dataset_batched,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=val_dataset_batched,\n",
    "                    callbacks=[reduce_lr,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 145.80\n",
      "Test accuracy: 58.50\n"
     ]
    }
   ],
   "source": [
    "aug_model = load_model('best_model.h5')\n",
    "test_dataset_batched = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "score = aug_model.evaluate(test_dataset_batched, verbose=0)\n",
    "print(f'Test loss: {score[0]*100:.2f}')\n",
    "print(f'Test accuracy: {score[1]*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
